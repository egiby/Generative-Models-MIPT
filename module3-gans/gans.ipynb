{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "badEjao5zR1o"
   },
   "source": [
    "# GANs\n",
    "\n",
    "This assignment consists of 4 parts, **to get full mark you should complete part 1, part 2 and one of any of the remaining parts**:\n",
    "1. 1D data (5 points)\n",
    "2. WGAN-GP on CIFAR-10 (15 points)\n",
    "3. \\*BiGAN on MNIST (10 points)\n",
    "4. \\*CycleGAN on ColoredMNIST (10 points)\n",
    "\n",
    "Each task is accompanied with some amount of code to allow you to concentrate on the most interesting parts of the assignment. You are free to modify any code in the solution section if you think it will make it more convenient. However you should not modify data acquisition and result sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "maU5VOGeNsge"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from scipy.stats import norm\n",
    "from tqdm import trange, tqdm_notebook\n",
    "import os.path as osp\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vYa4i0uxspaA"
   },
   "source": [
    "# Part 1: Warmup (5 points)\n",
    "\n",
    "In this question, we will train 2 different variants of GANs on an easy 1D dataset. \n",
    "\n",
    "Execute the cell below to visualize our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N7o80wZlKFKq"
   },
   "outputs": [],
   "source": [
    "def q1_data(n=20000):\n",
    "    assert n % 2 == 0\n",
    "    gaussian1 = np.random.normal(loc=-1, scale=0.25, size=(n//2,))\n",
    "    gaussian2 = np.random.normal(loc=0.5, scale=0.5, size=(n//2,))\n",
    "    data = (np.concatenate([gaussian1, gaussian2]) + 1).reshape([-1, 1])\n",
    "    scaled_data = (data - np.min(data)) / (np.max(data) - np.min(data) + 1e-8)\n",
    "    return 2 * scaled_data -1\n",
    "\n",
    "def visualize_q1_dataset():\n",
    "    data = q1_data()\n",
    "    plt.hist(data, bins=50, alpha=0.7, label='train data')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "colab_type": "code",
    "id": "Fb5MioT8SZzN",
    "outputId": "00f86f23-d733-4768-9428-421c28cdf641"
   },
   "outputs": [],
   "source": [
    "visualize_q1_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WSOkSmHSL2c3"
   },
   "source": [
    "## Part 1(a)\n",
    "In this part, we'll train our generator and discriminator via the original minimax GAN objective:\n",
    "<insert GAN Objective here>\n",
    "$$min_{G} max_{D} \\mathbb{E}_{x \\sim p_{data}} [\\log D(x)] + \\mathbb{E}_{z \\sim p(z)}[\\log (1-D(G(z)))]$$\n",
    "\n",
    "Use an MLP for both your generator and your discriminator, and train until the generated distribution resembles the target distribution. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rKl7kyUPwPSJ"
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eh3X-3vqMm2X"
   },
   "outputs": [],
   "source": [
    "def train(generator, critic, c_loss_fn, g_loss_fn, train_loader, g_optimizer, c_optimizer, n_critic=1, g_scheduler=None, c_scheduler=None, weight_clipping=None):\n",
    "    \"\"\"\n",
    "    generator: \n",
    "    critic: discriminator in 1ab, general model otherwise\n",
    "    c_loss_fn: takes (generator, discriminator, real_batch)\n",
    "    g_loss_fn: takes (generator, discriminator, real_batch)\n",
    "    train_loader: instance of DataLoader class\n",
    "    optimizer: \n",
    "    ncritic: how many critic gradient steps to do for every generator step\n",
    "    \"\"\"\n",
    "    g_losses, c_losses = [], []\n",
    "    generator.train()\n",
    "    critic.train()\n",
    "    for i, x in enumerate(train_loader):\n",
    "        x = x.to(device).float()\n",
    "\n",
    "        # discriminator/critic step\n",
    "        # TODO\n",
    "\n",
    "        if i % n_critic == 0:  # generator step\n",
    "            # TODO\n",
    "            if g_scheduler is not None:\n",
    "                g_scheduler.step()\n",
    "            if c_scheduler is not None:\n",
    "                c_scheduler.step()\n",
    "    return dict(g_losses=g_losses, c_losses=c_losses)\n",
    "\n",
    "def train_epochs(generator, critic, g_loss_fn, c_loss_fn, train_loader, train_args):\n",
    "    epochs, lr = train_args['epochs'], train_args['lr']\n",
    "    if 'optim_cls' in train_args:\n",
    "        g_optimizer = train_args['optim_cls'](generator.parameters(), lr=lr)\n",
    "        c_optimizer = train_args['optim_cls'](critic.parameters(), lr=lr)\n",
    "    else:\n",
    "        g_optimizer = optim.Adam(generator.parameters(), lr=lr, betas=(0, 0.9))\n",
    "        c_optimizer = optim.Adam(critic.parameters(), lr=lr, betas=(0, 0.9))\n",
    "        \n",
    "    if train_args.get('lr_schedule', None) is not None:\n",
    "        g_scheduler = optim.lr_scheduler.LambdaLR(g_optimizer, train_args['lr_schedule'])\n",
    "        c_scheduler = optim.lr_scheduler.LambdaLR(c_optimizer, train_args['lr_schedule'])\n",
    "    else:\n",
    "        g_scheduler = None\n",
    "        c_scheduler = None \n",
    "\n",
    "    train_losses = dict()\n",
    "    for epoch in tqdm_notebook(range(epochs), desc='Epoch', leave=False):\n",
    "        generator.train()\n",
    "        critic.train()\n",
    "        train_loss = train(generator, critic, c_loss_fn, g_loss_fn, train_loader, \n",
    "                           g_optimizer, c_optimizer, n_critic=train_args.get('n_critic', 0), \n",
    "                           g_scheduler=g_scheduler, c_scheduler=c_scheduler,\n",
    "                           weight_clipping=train_args.get('weight_clipping', None))\n",
    "        \n",
    "        for k in train_loss.keys():\n",
    "            if k not in train_losses:\n",
    "                train_losses[k] = []\n",
    "            train_losses[k].extend(train_loss[k])\n",
    "\n",
    "    if train_args.get('q1', False):  # q1, return the snapshots\n",
    "        final_snapshot = get_training_snapshot(generator, critic)\n",
    "        return [train_losses, *final_snapshot]\n",
    "    else:\n",
    "        return train_losses\n",
    "\n",
    "def get_training_snapshot(generator, critic, n_samples=5000):\n",
    "    generator.eval()\n",
    "    critic.eval()\n",
    "    xs = np.linspace(-1, 1, 1000)\n",
    "    samples = generator.sample(n_samples).detach().cpu().numpy()\n",
    "    critic_output = critic(torch.tensor(xs, device=device).float().unsqueeze(1)).detach().cpu().numpy()\n",
    "    return samples, xs, critic_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m8wZDEKoMt3m"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, n_hidden, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for _ in range(n_hidden):\n",
    "            layers.append(nn.Linear(input_size, hidden_size))\n",
    "            layers.append(nn.LeakyReLU(0.2))\n",
    "            input_size = hidden_size\n",
    "        layers.append(nn.Linear(hidden_size, output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class MLPGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim, n_hidden, hidden_size, data_dim):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        self.latent_dim = latent_dim\n",
    "        self.mlp = MLP(latent_dim, n_hidden, hidden_size, data_dim)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        # TODO: outputs in [-1, 1]\n",
    "        \n",
    "    def sample(self, n):\n",
    "        # n is the number of samples to return\n",
    "        # TODO\n",
    "\n",
    "class MLPDiscriminator(nn.Module):\n",
    "    def __init__(self, latent_dim, n_hidden, hidden_size, data_dim):\n",
    "        super().__init__()\n",
    "        self.mlp = MLP(latent_dim, n_hidden, hidden_size, data_dim)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        # TODO: output probabilities in [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pfyB3DFgKfA5"
   },
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "def q1_a(train_data):\n",
    "    \"\"\"\n",
    "    train_data: An (20000, 1) numpy array of floats in [-1, 1]\n",
    "\n",
    "    Returns\n",
    "    - a (# of training iterations,) numpy array of discriminator losses evaluated every minibatch\n",
    "\n",
    "    - a numpy array of size (5000,) of samples drawn from your model at the end of training\n",
    "    - a numpy array of size (1000,) linearly spaced from [-1, 1]; hint: np.linspace\n",
    "    - a numpy array of size (1000,), corresponding to the discriminator output (after sigmoid) \n",
    "        at each location in the previous array at the end of training\n",
    "    \"\"\"\n",
    "  \n",
    "    # create data loaders\n",
    "    train_loader = data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "    # model\n",
    "    g = MLPGenerator(1, 3, 128, 1).to(device)\n",
    "    c = MLPDiscriminator(1, 3, 128, 1).to(device)\n",
    "\n",
    "    # loss functions\n",
    "    def g_loss(generator, critic, x):\n",
    "        # TODO\n",
    "\n",
    "    def c_loss(generator, critic, x):\n",
    "        # TODO\n",
    "\n",
    "    # train\n",
    "    train_losses, samples, xs, ys = train_epochs(g, c, g_loss, c_loss, train_loader, dict(epochs=25, lr=1e-4, n_critic=2, q1=True))\n",
    "\n",
    "    return train_losses['c_losses'], samples, xs, ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KLrFM7TQwZkC"
   },
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gRisn-yGwqoO"
   },
   "outputs": [],
   "source": [
    "def plot_gan_training(losses, title, fname):\n",
    "    plt.figure()\n",
    "    n_itr = len(losses)\n",
    "    xs = np.arange(n_itr)\n",
    "\n",
    "    plt.plot(xs, losses, label='loss')\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Training Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    # savefig(fname)\n",
    "\n",
    "def q1_gan_plot(data, samples, xs, ys, title, fname):\n",
    "    plt.figure()\n",
    "    plt.hist(samples, bins=50, density=True, alpha=0.7, label='fake')\n",
    "    plt.hist(data, bins=50, density=True, alpha=0.7, label='real')\n",
    "\n",
    "    plt.plot(xs, ys, label='discrim')\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    # savefig(fname)\n",
    "\n",
    "def q1_save_results(part, fn):\n",
    "    data = q1_data()\n",
    "    losses, samples, xs, ys = fn(data)\n",
    "\n",
    "    # loss plot\n",
    "    plot_gan_training(losses, 'Q1{} Losses'.format(part), 'results/q1{}_losses.png'.format(part))\n",
    "\n",
    "    # samples\n",
    "    q1_gan_plot(data, samples, xs, ys, 'Q1{} Final'.format(part), 'results/q1{}_final.png'.format(part))\n",
    "\n",
    "q1_save_results('a', q1_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2x4hfHRbZrDT"
   },
   "source": [
    "## Part 1(b)\n",
    "Here, we'll use the non-saturating formulation of the GAN objective. Now, we have two separate losses:\n",
    "$$L^{(D)} = \\mathbb{E}_{x \\sim p_{data}} [\\log D(x)] + \\mathbb{E}_{z \\sim p(z)}[\\log (1-D(G(z)))]$$\n",
    "$$L^{(G} = - \\mathbb{E}_{z \\sim p(z)} \\log(D(G(z))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Uun1MzfwpuC"
   },
   "source": [
    " ### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_qARVcqpQ014"
   },
   "outputs": [],
   "source": [
    "def q1_b(train_data):\n",
    "    # create data loaders\n",
    "    train_loader = data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "    # model\n",
    "    g = MLPGenerator(1, 3, 128, 1).to(device)\n",
    "    c = MLPDiscriminator(1, 3, 128, 1).to(device)\n",
    "\n",
    "    # loss functions\n",
    "    def g_loss(generator, critic, x):\n",
    "        # TODO\n",
    "\n",
    "    def c_loss(generator, critic, x):\n",
    "        # TODO\n",
    "\n",
    "    # train\n",
    "    train_losses, samples, xs, ys = train_epochs(g, c, g_loss, c_loss, train_loader, dict(epochs=25, lr=1e-4, n_critic=2, q1=True))\n",
    "\n",
    "    return train_losses['c_losses'], samples, xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MJeEgn6zZst0"
   },
   "outputs": [],
   "source": [
    "def q1_b(train_data):\n",
    "    \"\"\"\n",
    "    train_data: An (20000, 1) numpy array of floats in [-1, 1]\n",
    "\n",
    "    Returns\n",
    "    - a (# of training iterations,) numpy array of discriminator losses evaluated every minibatch\n",
    "\n",
    "    - a numpy array of size (5000,) of samples drawn from your model at the end of training\n",
    "    - a numpy array of size (1000,) linearly spaced from [-1, 1]; hint: np.linspace\n",
    "    - a numpy array of size (1000,), corresponding to the discriminator output (after sigmoid) \n",
    "      at each location in the previous array at the end of training\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\" YOUR CODE HERE \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b0VSrZWzwrzT"
   },
   "source": [
    " ### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xfv-DeVKwtXl"
   },
   "outputs": [],
   "source": [
    "q1_save_results('b', q1_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sBESzChmEfcF"
   },
   "source": [
    "# Part 2: GANs on CIFAR-10 (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZLRdpUxy5jc0"
   },
   "source": [
    "In this exercise, you will train GANs on CIFAR-10. Execute the cell below to visualize the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "id": "-WbafudL5mnz",
    "outputId": "026197ea-ef50-4b77-cbeb-73f2af85b20f"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "def show_samples(samples, fname=None, nrow=10, title='Samples'):\n",
    "    samples = (torch.FloatTensor(samples) / 255).permute(0, 3, 1, 2)\n",
    "    grid_img = make_grid(samples, nrow=nrow)\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.imshow(grid_img.permute(1, 2, 0))\n",
    "    plt.axis('off')\n",
    "\n",
    "    if fname is not None:\n",
    "        savefig(fname)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def load_q2_data():\n",
    "    train_data = torchvision.datasets.CIFAR10(\"./data\", transform=torchvision.transforms.ToTensor(),\n",
    "                                              download=True, train=True)\n",
    "    return train_data\n",
    "\n",
    "def visualize_q2_data():\n",
    "    train_data = load_q2_data()\n",
    "    imgs = train_data.data[:100]\n",
    "    show_samples(imgs, title=f'CIFAR-10 Samples')\n",
    "\n",
    "visualize_q2_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vIYRnVafEkcd"
   },
   "source": [
    " We'll use the CIFAR-10 architecture from the [SN-GAN paper](https://arxiv.org/pdf/1802.05957.pdf), with $z \\in \\mathbb R ^{128}$, with $z \\sim \\mathcal N (0, I_{128})$. Instead of upsampling via transposed convolutions and downsampling via pooling or striding, we'll use these DepthToSpace and SpaceToDepth methods for changing the spatial configuration of our hidden states. \n",
    "\n",
    "```\n",
    "class DepthToSpace(nn.Module):\n",
    "    def __init__(self, block_size):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.block_size_sq = block_size * block_size\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = input.permute(0, 2, 3, 1)\n",
    "        (batch_size, d_height, d_width, d_depth) = output.size()\n",
    "        s_depth = int(d_depth / self.block_size_sq)\n",
    "        s_width = int(d_width * self.block_size)\n",
    "        s_height = int(d_height * self.block_size)\n",
    "        t_1 = output.reshape(batch_size, d_height, d_width, self.block_size_sq, s_depth)\n",
    "        spl = t_1.split(self.block_size, 3)\n",
    "        stack = [t_t.reshape(batch_size, d_height, s_width, s_depth) for t_t in spl]\n",
    "        output = torch.stack(stack, 0).transpose(0, 1).permute(0, 2, 1, 3, 4).reshape(batch_size, s_height, s_width,\n",
    "                                                                                      s_depth)\n",
    "        output = output.permute(0, 3, 1, 2)\n",
    "        return output\n",
    "\n",
    "\n",
    "class SpaceToDepth(nn.Module):\n",
    "    def __init__(self, block_size):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.block_size_sq = block_size * block_size\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = input.permute(0, 2, 3, 1)\n",
    "        (batch_size, s_height, s_width, s_depth) = output.size()\n",
    "        d_depth = s_depth * self.block_size_sq\n",
    "        d_width = int(s_width / self.block_size)\n",
    "        d_height = int(s_height / self.block_size)\n",
    "        t_1 = output.split(self.block_size, 2)\n",
    "        stack = [t_t.reshape(batch_size, d_height, d_depth) for t_t in t_1]\n",
    "        output = torch.stack(stack, 1)\n",
    "        output = output.permute(0, 2, 1, 3)\n",
    "        output = output.permute(0, 3, 1, 2)\n",
    "        return output\n",
    "\n",
    "# Spatial Upsampling with Nearest Neighbors\n",
    "Upsample_Conv2d(in_dim, out_dim, kernel_size=(3, 3), stride=1, padding=1):\n",
    "    x = torch.cat([x, x, x, x], dim=1)\n",
    "    DepthToSpace(block_size=2)\n",
    "    Conv2d(in_dim, out_dim, kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "\n",
    "# Spatial Downsampling with Spatial Mean Pooling\n",
    "Downsample_Conv2d(in_dim, out_dim, kernel_size=(3, 3), stride=1, padding=1):\n",
    "        SpaceToDepth(2)\n",
    "        torch.sum(x.chunk(4, dim=1)) / 4.0\n",
    "        nn.Conv2d(in_dim, out_dim, kernel_size,\n",
    "                              stride=stride, padding=padding, bias=bias)\n",
    "```\n",
    "\n",
    "Here's pseudocode for how we'll implement a ResBlockUp, used in the generator:\n",
    "\n",
    "```\n",
    "ResnetBlockUp(x, in_dim, kernel_size=(3, 3), n_filters=256):\n",
    "    _x = x\n",
    "    _x = nn.BatchNorm2d(in_dim)(_x)\n",
    "    _x = nn.ReLU()(_x)\n",
    "    _x = nn.Conv2d(in_dim, n_filters, kernel_size, padding=1)(_x)\n",
    "    _x = nn.BatchNorm2d(n_filters)(_x)\n",
    "    _x = nn.ReLU()(_x)\n",
    "    residual = Upsample_Conv2d(n_filters, n_filters, kernel_size, padding=1)(_x)\n",
    "    shortcut = Upsample_Conv2d(in_dim, n_filters, kernel_size=(1, 1), padding=0)(x)\n",
    "    return residual + shortcut\n",
    "```\n",
    "The ResBlockDown module is similar, except it uses Downsample_Conv2d and omits the BatchNorm.\n",
    "\n",
    "Finally, here's the architecture for the generator:\n",
    "```\n",
    "def Generator(*, n_samples=1024, n_filters=128):\n",
    "    z = Normal(0, 1)([n_samples, 128])\n",
    "    nn.Linear(128, 4*4*256)\n",
    "    reshape output of linear layer\n",
    "    ResnetBlockUp(in_dim=256, n_filters=n_filters),\n",
    "    ResnetBlockUp(in_dim=n_filters, n_filters=n_filters),\n",
    "    ResnetBlockUp(in_dim=n_filters, n_filters=n_filters),\n",
    "    nn.BatchNorm2d(n_filters),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(n_filters, 3, kernel_size=(3, 3), padding=1),\n",
    "    nn.Tanh()\n",
    "```\n",
    "Again, the discriminator has the same architecture, except with ResnetBlockDown and no BatchNorm.\n",
    "\n",
    "**Hyperparameters**\n",
    "\n",
    "We'll implement [WGAN-GP](https://arxiv.org/abs/1704.00028), which uses a gradient penalty to regularize the discriminator. Use the Adam optimizer with $\\alpha = 2e-4$, $\\beta_1 = 0$, $\\beta_2 = 0.9$, $\\lambda = 10$, $n_{critic} = 5$. Use a batch size of 256 and n_filters=128 within the ResBlocks. Train for at least 25000 (**Warning: 25000 g steps will take ~12 hours on colab, so consider starting from 2500 steps and if your generator converges to something reasonable you can proceed**) gradient steps, with the learning rate linearly annealed to 0 over training. \n",
    "\n",
    "**You will provide the following deliverables**\n",
    "1. Inception score (CIFAR-10 version) of the final model. We provide a utility that will automatically do this for you.\n",
    "3. Discriminator loss across training\n",
    "4. 100 samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WlY4YYyedBlR"
   },
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8zZnCcdgUoqw"
   },
   "outputs": [],
   "source": [
    "# TODO: write layers described above\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, n_filters=256):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc = nn.Linear(128, 4 * 4 * 256)\n",
    "        network = [\n",
    "            # TODO\n",
    "        ]\n",
    "        self.net = nn.Sequential(*network)\n",
    "        self.noise = torch.distributions.Normal(torch.tensor(0.), torch.tensor(1.))\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = self.fc(z).reshape(-1, 256, 4, 4)\n",
    "        return self.net(z)\n",
    "\n",
    "    def sample(self, n_samples):\n",
    "        z = self.noise.sample([n_samples, 128]).to(device)\n",
    "        return self.forward(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc = nn.Linear(128, 4 * 4 * 256)\n",
    "        network = [\n",
    "            # TODO\n",
    "        ]\n",
    "        self.net = nn.Sequential(*network)\n",
    "        self.fc = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = self.net(z)\n",
    "        z = torch.sum(z, dim=(2, 3))\n",
    "        return self.fc(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RMu-l46NUo08"
   },
   "outputs": [],
   "source": [
    "class Solver(object):\n",
    "    def __init__(self, train_data, n_iterations=50000, batch_size=256, n_filters=25000):\n",
    "        self.n_critic = 5\n",
    "        self.log_interval = 100\n",
    "        self.batch_size = batch_size\n",
    "        self.n_filters = n_filters\n",
    "        self.train_loader = self.create_loaders(train_data)\n",
    "        self.n_batches_in_epoch = len(self.train_loader)\n",
    "        self.n_epochs = self.n_critic * n_iterations // self.n_batches_in_epoch\n",
    "        self.curr_itr = 0\n",
    "\n",
    "    def build(self, part_name):\n",
    "        self.g = Generator(n_filters=self.n_filters).to(device)\n",
    "        self.d = Discriminator().to(device)\n",
    "        self.g_optimizer = # TODO (do not forget to use recommended parameters, it is very important)\n",
    "        self.g_scheduler = # TODO\n",
    "        self.d_optimizer = # TODO\n",
    "        self.d_scheduler = # TODO\n",
    "        self.part_name = part_name\n",
    "\n",
    "    def create_loaders(self, train_data):\n",
    "        train_loader = data.DataLoader(train_data, batch_size=self.batch_size, shuffle=True)\n",
    "        return train_loader\n",
    "\n",
    "    def gradient_penalty(self, real_data, fake_data):\n",
    "        # TODO\n",
    "\n",
    "    def train(self):\n",
    "        train_losses = []\n",
    "        for epoch_i in tqdm_notebook(range(self.n_epochs), desc='Epoch', leave=False):\n",
    "            epoch_i += 1\n",
    "\n",
    "            self.d.train()\n",
    "            self.g.train()\n",
    "            self.batch_loss_history = []\n",
    "\n",
    "            for batch_i, x in enumerate(tqdm_notebook(self.train_loader, desc='Batch', leave=False)):\n",
    "                batch_i += 1\n",
    "                self.curr_itr += 1\n",
    "                x = torch.tensor(x).float().to(device)\n",
    "                x = 2 * (x - 0.5)\n",
    "\n",
    "                # do a critic update\n",
    "                # TODO\n",
    "                # generator update\n",
    "                if self.curr_itr % self.n_critic == 0:\n",
    "                    # TODO\n",
    "\n",
    "                    # step the learning rate\n",
    "                    self.g_scheduler.step()\n",
    "                    self.d_scheduler.step()\n",
    "\n",
    "                    self.batch_loss_history.append(g_loss.data.cpu().numpy())\n",
    "\n",
    "            epoch_loss = np.mean(self.batch_loss_history)\n",
    "            train_losses.append(epoch_loss)\n",
    "            np.save(\"q2_train_losses.npy\", np.array(train_losses))\n",
    "\n",
    "        train_losses = np.array(train_losses)\n",
    "        self.save_model(f\"{self.part_name}.pt\")\n",
    "        return train_losses\n",
    "\n",
    "    def sample_for_eval(self, n_samples):\n",
    "        # should return numpy images\n",
    "        # TODO\n",
    "\n",
    "    def save_model(self, filename):\n",
    "        split_path = list(osp.split(filename))\n",
    "        g_path = osp.join(*split_path[:-1], 'g_' + split_path[-1])\n",
    "        d_path = osp.join(*split_path[:-1], 'd_' + split_path[-1])\n",
    "        torch.save(self.g.state_dict(), g_path)\n",
    "        torch.save(self.d.state_dict(), d_path)\n",
    "\n",
    "    def load_model(self, filename):\n",
    "        split_path = list(osp.split(filename))\n",
    "        g_path = osp.join(*split_path[:-1], 'g_' + split_path[-1])\n",
    "        d_path = osp.join(*split_path[:-1], 'd_' + split_path[-1])\n",
    "        self.d.load_state_dict(torch.load(d_path))\n",
    "        self.g.load_state_dict(torch.load(g_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5bQPfEKiVF_B"
   },
   "outputs": [],
   "source": [
    "def q2(train_data, model_path=None, losses_path=None, n_iterations=25000):\n",
    "    \"\"\"\n",
    "    train_data: An (n_train, 3, 32, 32) numpy array of CIFAR-10 images with values in [0, 1]\n",
    "\n",
    "    Returns\n",
    "    - a (# of training iterations,) numpy array of WGAN critic train losses evaluated every minibatch\n",
    "    - a (1000, 32, 32, 3) numpy array of samples from your model in [0, 1]. \n",
    "        The first 100 will be displayed, and the rest will be used to calculate the Inception score. \n",
    "    \"\"\"\n",
    "    solver = Solver(train_data, n_iterations=n_iterations)\n",
    "    solver.build(\"wgan\")\n",
    "    if model_path is not None and losses_path is not None:\n",
    "        solver.load_model(model_path)\n",
    "        losses = np.load(losses_path)\n",
    "    else:\n",
    "        losses = solver.train()\n",
    "    solver.g.eval()\n",
    "    solver.d.eval()\n",
    "    with torch.no_grad():\n",
    "        samples = solver.g.sample(1000)\n",
    "        samples = samples.permute(0, 2, 3, 1).detach().cpu().numpy() * 0.5 + 0.5\n",
    "\n",
    "    return losses, samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UydRmPpLdEar"
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "32QNJPgJwu_i"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import math\n",
    "import sys\n",
    "from matplotlib.pyplot import savefig\n",
    "\n",
    "def calculate_is(samples):\n",
    "    assert (type(samples[0]) == np.ndarray)\n",
    "    assert (len(samples[0].shape) == 3)\n",
    "\n",
    "    model = torchvision.models.googlenet(pretrained=True).to(device)\n",
    "\n",
    "    bs = 100\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = []\n",
    "        n_batches = int(math.ceil(float(len(samples)) / float(bs)))\n",
    "        for i in range(n_batches):\n",
    "            sys.stdout.write(\".\")\n",
    "            sys.stdout.flush()\n",
    "            inp = torch.FloatTensor(samples[(i * bs):min((i + 1) * bs, len(samples))]).to(device)\n",
    "            pred = F.softmax(model(inp), dim=1).detach().cpu().numpy()\n",
    "            preds.append(pred)\n",
    "    preds = np.concatenate(preds, 0)\n",
    "    kl = preds * (np.log(preds) - np.log(np.expand_dims(np.mean(preds, 0), 0)))\n",
    "    kl = np.mean(np.sum(kl, 1))\n",
    "    return np.exp(kl)\n",
    "\n",
    "def q2_save_results(fn):\n",
    "    train_data = load_q2_data()\n",
    "    train_data = train_data.data.transpose((0, 3, 1, 2)) / 255.0\n",
    "    train_losses, samples = fn(train_data)\n",
    "\n",
    "    print(\"Inception score:\", calculate_is(samples.transpose([0, 3, 1, 2])))\n",
    "    plot_gan_training(train_losses, 'Q2 Losses', 'results/q2_losses.png')\n",
    "    show_samples(samples[:100] * 255.0, fname=None, title=f'CIFAR-10 generated samples')\n",
    "\n",
    "q2_save_results(q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j90EZiaJtqPM"
   },
   "source": [
    "# Part 3: Representation Learning with BiGAN on MNIST (10 points)\n",
    "\n",
    "In this part, we train a BiGAN on the MNIST dataset. Execute the cell below to visualize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 647,
     "referenced_widgets": [
      "21a3795bb6c44aa2b2f5de9ccb667bb6",
      "77360e5cb1324395831f617a74a14d31",
      "1e4b7dffd7674663aa4317d96e07a425",
      "97c3c0371b43488ba5e628e2aea1fff9",
      "537d532842b34fc28e7fc11f2d458c83",
      "4b0dbebfb55e409fa1439e9b41afae66",
      "cefde845a29c442dbdd206067fce84ce",
      "4fed3c34233c4e2b8e06fe3d8b2b3c8b",
      "33f612e221bd4ac3a4eba2bdd6493d04",
      "e13c2483c2db47dabfcbc3a7a6de4b35",
      "53be47757c9941b18366b4d8373cc40a",
      "ed226d6072ae4a65b1b789912593dae5",
      "690de1ab3502431d988da8ad8399c838",
      "ae5a8f4cb4d1464e9795f3dbf7dd8691",
      "c21f4bf579dd47f3b33c84fc76ed3dc9",
      "14b8fe0143c44ab890480cf61fcee8cc",
      "f3955b7b0b04470990dab09f9f36dd1a",
      "caca3df014bb4f1aadac830ac6900e4c",
      "e5a15d3d384e4c0bb0e6d4c6722a4f08",
      "9859004091154a18b436fd29a6870aec",
      "95aeb90ff17e42d5bf2c1cd0451f2d68",
      "dc5594291f7a4e27a7820d2402db7254",
      "0ee102f0e64e4134b7b4197e0ffed618",
      "b21abaaeac8144af9ff5661d320fc457",
      "553d87b223404f3698ece12862c6ccd7",
      "037b67dcc11940a5ad2c38b830faf71f",
      "b4c12d0ef09c41b79000af4573de594b",
      "b34330df934d470f9f1a669c77160386",
      "9f6fd397241645539b88e7cbd2f33dbb",
      "c597443e923b496ab5a868c0f11cc64b",
      "149ae0a3a02b4f24850539870f170f64",
      "73f63317156a49cc96eba50e275b4fac"
     ]
    },
    "colab_type": "code",
    "id": "Ltb-rJSnt_3D",
    "outputId": "dd20d92a-8543-4c5e-ddd0-a4980dc8b96b"
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "def load_q3_data():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    train_data = torchvision.datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "    test_data = torchvision.datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "    return train_data, test_data\n",
    "\n",
    "def visualize_q3_data():\n",
    "    train_data, _ = load_q3_data()\n",
    "    imgs = train_data.data[:100]\n",
    "    show_samples(imgs.reshape([100, 28, 28, 1]) * 255.0, title=f'MNIST samples')\n",
    "\n",
    "visualize_q3_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D9LvBaKetyXN"
   },
   "source": [
    "In BiGAN, in addition to training a generator $G$ and a discriminator $D$, we train an encoder $E$ that maps from real images $x$ to latent codes $z$. The discriminator now must learn to jointly identify fake $z$, fake $x$, and paired $(x, z)$ that don't belong together. In the original [BiGAN paper](https://arxiv.org/pdf/1605.09782.pdf), they prove that the optimal $E$ learns to invert the generative mapping $G: z \\rightarrow x$. Our overall minimax term is now\n",
    "$$V(D, E, G) = \\mathbb{E}_{x \\sim p_x}[\\mathbb{E}_{z \\sim p_E(\\cdot | x)}[\\log D(x, z)]] + \\mathbb{E}_{z \\sim p_z}[\\mathbb{E}_{x \\sim p_G(\\cdot | z)}[\\log (1 - D(x, z))]]$$\n",
    "\n",
    "**Architecture**\n",
    "\n",
    "We will closely follow the MNIST architecture outlined in the original BiGAN paper, Appendix C.1, with one modification: instead of having $z \\sim \\text{Uniform}[-1, 1]$, we use $z \\sim \\mathcal N (0, 1)$ with $z \\in \\mathbb R ^{50}$. \n",
    "\n",
    "**Hyperparameters**\n",
    "\n",
    "We make several modifications to what is listed in the BiGAN paper. We apply $l_2$ weight decay to all weights and decay the step size $\\alpha$ linearly to 0 over the course of training. Weights are initialized via the default PyTorch manner. We recommend training for at least 100 epochs -- this part is much less computationally expensive than previous homeworks. \n",
    "\n",
    "**Reconstructions**\n",
    "\n",
    "You should take the first 20 images from the MNIST training set and then display the reconstructions $x_{recon} = G(E(x))$. It's ok if your reconstructions are somewhat lossy; ours are too. We will provide a utility to show these. \n",
    "\n",
    "**Testing the representation**\n",
    "\n",
    "We want to see how good a linear classifier $L$ we can learn such that \n",
    "$$y \\approx L(E(x))$$\n",
    "where $y$ is the appropriate label. Fix $E$ and learn a weight matrix $W$ such that your linear classifier is composed of passing $x$ through $E$, then multiplying by $W$, then applying a softmax nonlinearity. Train this via gradient descent with the cross-entropy loss. \n",
    "\n",
    "As a baseline, randomly initialize another network $E_{random}$ with the same architecture, fix its weights, and train a linear classifier on top, as done in the previous part.\n",
    "\n",
    "\n",
    "**You will provide the following deliverables**\n",
    "1. Plot of the minimax loss term. \n",
    "2. 100 samples from the BiGAN\n",
    "3. A comparison of MNIST images $x$ with their reconstructions $G(E(x))$.\n",
    "4. Test loss plot for the linear classifier trained on the pretrained encoder, and a randomly initialized encoder. \n",
    "5. Final test losses for the two linear classifiers. \n",
    "\n",
    "**Feel free to train the networks separately, save the necessary information, and simply use our function to generate the appropriate figures.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RZTjJOOodhwq"
   },
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HJJ-c3IdZ27W"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, g_input_dim, g_output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            # TODO\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x).reshape(x.shape[0], 1, 28, 28)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, z_dim, x_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            # TODO\n",
    "        )\n",
    "\n",
    "    def forward(self, z, x):\n",
    "        x = torch.cat((z, x), dim=1)\n",
    "        return self.fc(x)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            # TODO\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u1aBihquZ4DY"
   },
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "\n",
    "class Solver(object):\n",
    "    def __init__(self, train_data, test_data, n_epochs=100, batch_size=128, latent_dim=50):\n",
    "        self.log_interval = 100\n",
    "        self.batch_size = batch_size\n",
    "        self.train_loader, self.test_loader = self.create_loaders(train_data, test_data)\n",
    "        self.n_batches_in_epoch = len(self.train_loader)\n",
    "        self.n_epochs = n_epochs\n",
    "        self.curr_itr = 0\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def build(self):\n",
    "        # BiGAN \n",
    "        self.d = Discriminator(self.latent_dim, 784).to(device)\n",
    "        self.e = Encoder(784, self.latent_dim).to(device) \n",
    "        self.g = Generator(self.latent_dim, 784).to(device)\n",
    "        self.g_optimizer = # TODO\n",
    "        self.g_scheduler = # TODO\n",
    "        self.d_optimizer = # TODO\n",
    "        self.d_scheduler = # TODO\n",
    "        \n",
    "        # linear classifier \n",
    "        self.linear = nn.Linear(self.latent_dim, 10).to(device)\n",
    "        self.linear_optimizer = torch.optim.Adam(self.linear.parameters(), lr=1e-3)\n",
    "\n",
    "    def reset_linear(self):\n",
    "        self.linear = nn.Linear(self.latent_dim, 10).to(device)\n",
    "        self.linear_optimizer = torch.optim.Adam(self.linear.parameters(), lr=1e-3)\n",
    "\n",
    "    def create_loaders(self, train_data, test_data):\n",
    "        train_loader = torch.utils.data.DataLoader(train_data, batch_size=self.batch_size, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(test_data, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        return train_loader, test_loader\n",
    "\n",
    "    def get_discriminator_loss(self, x):\n",
    "        z_fake = torch.normal(torch.zeros(x.shape[0], self.latent_dim), torch.ones(x.shape[0], self.latent_dim)).to(device)\n",
    "        z_real = self.e(x).reshape(x.shape[0], self.latent_dim)\n",
    "        x_fake = self.g(z_fake).reshape(x.shape[0], -1)\n",
    "        x_real = x.view(x.shape[0], -1)\n",
    "\n",
    "        d_loss = - 0.5 * (self.d(z_real, x_real)).log().mean() - 0.5 * (1 - self.d(z_fake, x_fake)).log().mean()\n",
    "        return d_loss\n",
    "\n",
    "\n",
    "    def train_bigan(self):\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        for epoch_i in tqdm_notebook(range(self.n_epochs), desc='Epoch'):\n",
    "            epoch_i += 1\n",
    "\n",
    "            self.d.train()\n",
    "            self.g.train()\n",
    "            self.e.train()\n",
    "            self.batch_loss_history = []\n",
    "\n",
    "            for batch_i, (x, y) in enumerate(tqdm_notebook(self.train_loader, desc='Batch', leave=False)):\n",
    "                batch_i += 1\n",
    "                self.curr_itr += 1\n",
    "                x = x.to(device).float()\n",
    "\n",
    "                # discriminator update\n",
    "                # TODO\n",
    "\n",
    "                # generator and encoder update\n",
    "                # TODO\n",
    "\n",
    "                self.batch_loss_history.append(d_loss.item())\n",
    "\n",
    "            # step the learning rate\n",
    "            self.g_scheduler.step()\n",
    "            self.d_scheduler.step()\n",
    "            epoch_loss = np.mean(self.batch_loss_history)\n",
    "            train_losses.append(epoch_loss)\n",
    "            self.save_samples(100, f'epoch{epoch_i}_samples.png')\n",
    "\n",
    "        np.save(\"train_losses.npy\", np.array(train_losses))\n",
    "        self.save_models('weights.pt')\n",
    "        train_losses = np.array(train_losses)\n",
    "\n",
    "        return train_losses\n",
    "\n",
    "    def train_linear_classifier(self):  # (everything already implemented)\n",
    "        train_losses = []\n",
    "        val_accs = []\n",
    "        for epoch_i in tqdm_notebook(range(self.n_epochs // 4), desc='Epoch'):\n",
    "            epoch_i += 1\n",
    "\n",
    "            self.e.eval()\n",
    "            self.linear.train()\n",
    "            self.batch_loss_history = []\n",
    "\n",
    "            for batch_i, (x, y) in enumerate(tqdm_notebook(self.train_loader, desc='Batch', leave=False)):\n",
    "                batch_i += 1\n",
    "                self.curr_itr += 1\n",
    "                x = x.to(device).float() # * 2 - 1\n",
    "                y = y.to(device)\n",
    "\n",
    "                # calculate loss, take gradient step\n",
    "                self.linear_optimizer.zero_grad()\n",
    "                z = self.e(x).detach()\n",
    "                pred = self.linear(z)\n",
    "                linear_loss = F.cross_entropy(pred, y)\n",
    "                linear_loss.backward()\n",
    "                self.linear_optimizer.step()\n",
    "\n",
    "                self.batch_loss_history.append(linear_loss.item())\n",
    "\n",
    "            val_acc = self.val_acc()\n",
    "            val_accs.append(val_acc)\n",
    "            epoch_loss = np.mean(self.batch_loss_history)\n",
    "            train_losses.append(epoch_loss)\n",
    "\n",
    "        np.save(\"train_losses.npy\", np.array(train_losses))\n",
    "        self.save_models('weights.pt')\n",
    "        train_losses = np.array(train_losses)\n",
    "        val_accs = np.array(val_accs)\n",
    "\n",
    "        return train_losses, val_accs\n",
    "\n",
    "    def sample(self, n):\n",
    "        # TODO\n",
    "\n",
    "    def save_samples(self, n, filename):\n",
    "        self.g.eval()\n",
    "        with torch.no_grad():\n",
    "            z = (torch.rand(n, self.latent_dim).to(device) - 0.5) * 2\n",
    "            samples = self.g(z).reshape(-1, 1, 28, 28) * 0.5 + 1\n",
    "            save_image(samples, filename, nrow=10, normalize=True)\n",
    "\n",
    "    def val_acc(self):\n",
    "        self.e.eval()\n",
    "        self.linear.eval()\n",
    "\n",
    "        val_acc_total = 0\n",
    "        val_items = 0\n",
    "        with torch.no_grad():\n",
    "            for (inputs, labels) in self.test_loader:\n",
    "                inputs = inputs.to(device).float()\n",
    "                z = self.e(inputs)\n",
    "                labels = labels.to(device)\n",
    "                logits = self.linear(z)\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                num_correct = torch.sum(predictions == labels).float()\n",
    "                val_acc_total += num_correct\n",
    "                val_items += inputs.shape[0]\n",
    "\n",
    "        return (val_acc_total / val_items).detach().cpu().numpy()\n",
    "\n",
    "    def save_models(self, filename):\n",
    "        torch.save(self.g.state_dict(), \"g_\" + filename)\n",
    "        torch.save(self.d.state_dict(), \"d_\" + filename)\n",
    "        torch.save(self.e.state_dict(), \"e_\" + filename)\n",
    "\n",
    "    def load_models(self, filename):\n",
    "        self.g.load_state_dict(torch.load(\"g_\" + filename))\n",
    "        self.d.load_state_dict(torch.load(\"d_\" + filename))\n",
    "        self.e.load_state_dict(torch.load(\"e_\" + filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TA7agDH5aBxH"
   },
   "outputs": [],
   "source": [
    "def q3(train_data, test_data):\n",
    "    \"\"\"\n",
    "    train_data: A PyTorch dataset that contains (n_train, 28, 28) MNIST digits, normalized to [-1, 1]\n",
    "                Documentation can be found at torchvision.datasets.MNIST\n",
    "    test_data: A PyTorch dataset that contains (n_test, 28, 28) MNIST digits, normalized to [-1, 1]\n",
    "                Documentation can be found at torchvision.datasets.MNIST\n",
    "\n",
    "    Returns\n",
    "    - a (# of training iterations,) numpy array of BiGAN minimax losses evaluated every minibatch\n",
    "    - a (100, 28, 28, 1) numpy array of BiGAN samples that lie in [0, 1]\n",
    "    - a (40, 28, 28, 1) numpy array of 20 real image / reconstruction pairs\n",
    "    - a (# of training epochs,) numpy array of supervised cross-entropy losses on the BiGAN encoder evaluated every epoch \n",
    "    - a (# of training epochs,) numpy array of supervised cross-entropy losses on a random encoder evaluated every epoch \n",
    "    \"\"\"\n",
    "\n",
    "    solver = Solver(train_data, test_data, n_epochs=100)\n",
    "    solver.build()\n",
    "\n",
    "    # get random encoder accuracy\n",
    "    print(\"Training linear classifier on random encoder\")\n",
    "    train_losses, val_accs = solver.train_linear_classifier()\n",
    "\n",
    "    # train bigan\n",
    "    print(\"Training BiGAN\")\n",
    "    bigan_losses = solver.train_bigan()\n",
    "    samples = # TODO\n",
    "    train_images = # TODO\n",
    "    recons = # TODO reconstructions(train_images)\n",
    "\n",
    "    # see benefit of unsupervised learning\n",
    "    solver.reset_linear()\n",
    "    print(\"Training linear classifier on BiGAN encoder\")\n",
    "    bigan_train_losses, bigan_val_accs = solver.train_linear_classifier()\n",
    "\n",
    "    print(f\"Final BiGAN test linear accuracy: {bigan_val_accs[-1]}\")\n",
    "    print(f\"Final random encoder test linear accuracy: {val_accs[-1]}\")\n",
    "\n",
    "    return bigan_losses, samples, np.concatenate([train_images, recons], axis=0).transpose(0, 2, 3, 1), bigan_train_losses, train_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HhXu2KrAdj0T"
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5FGKlCLQdkpG"
   },
   "outputs": [],
   "source": [
    "def plot_q3_supervised(pretrained_losses, random_losses, title, fname):\n",
    "    plt.figure()\n",
    "    xs = np.arange(len(pretrained_losses))\n",
    "    plt.plot(xs, pretrained_losses, label='bigan')\n",
    "    xs = np.arange(len(random_losses))\n",
    "    plt.plot(xs, random_losses, label='random init')\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    # savefig(fname)\n",
    "\n",
    "def q3_save_results(fn):\n",
    "    train_data, test_data = load_q3_data()\n",
    "    gan_losses, samples, reconstructions, pretrained_losses, random_losses = fn(train_data, test_data)\n",
    "\n",
    "    plot_gan_training(gan_losses, 'Q3 Losses', 'results/q3_gan_losses.png')\n",
    "    plot_q3_supervised(pretrained_losses, random_losses, 'Linear classification losses', 'results/q3_supervised_losses.png')\n",
    "    show_samples(samples * 255.0, fname=None, title='BiGAN generated samples')\n",
    "    show_samples(reconstructions * 255.0, nrow=20, fname=None, title=f'BiGAN reconstructions')\n",
    "    print('BiGAN final linear classification loss:', pretrained_losses[-1])\n",
    "    print('Random encoder linear classification loss:', random_losses[-1])\n",
    "\n",
    "q3_save_results(q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "92f0jUFadqml"
   },
   "source": [
    "# Part 4: CycleGAN (10 points)\n",
    "In this question, you'll train a CycleGAN model to learn to translate between two different image domains, without any paired data. Execute the following cell to visualize our two datasets: MNIST and Colored MNIST. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "Zul9AjeZwW4q",
    "outputId": "900739c5-48dc-4c00-d9cd-851cd9991adb"
   },
   "outputs": [],
   "source": [
    "!wget https://upload.wikimedia.org/wikipedia/en/7/7d/Lenna_%28test_image%29.png -O lena.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 511
    },
    "colab_type": "code",
    "id": "Y3qFm_JfObPj",
    "outputId": "6c3b3794-630f-4f48-882f-9f9fe4b13fb7"
   },
   "outputs": [],
   "source": [
    "import PIL\n",
    "import cv2\n",
    "import scipy\n",
    "\n",
    "def get_colored_mnist(data):\n",
    "    # from https://www.wouterbulten.nl/blog/tech/getting-started-with-gans-2-colorful-mnist/\n",
    "    # Read Lena image\n",
    "    lena = PIL.Image.open('./lena.jpg')\n",
    "\n",
    "    # Resize\n",
    "    batch_resized = np.asarray([scipy.ndimage.zoom(image, (2.3, 2.3, 1), order=1) for image in data])\n",
    "\n",
    "    # Extend to RGB\n",
    "    batch_rgb = np.concatenate([batch_resized, batch_resized, batch_resized], axis=3)\n",
    "\n",
    "    # Make binary\n",
    "    batch_binary = (batch_rgb > 0.5)\n",
    "\n",
    "    batch = np.zeros((data.shape[0], 28, 28, 3))\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "        # Take a random crop of the Lena image (background)\n",
    "        x_c = np.random.randint(0, lena.size[0] - 64)\n",
    "        y_c = np.random.randint(0, lena.size[1] - 64)\n",
    "        image = lena.crop((x_c, y_c, x_c + 64, y_c + 64))\n",
    "        image = np.asarray(image) / 255.0\n",
    "\n",
    "        # Invert the colors at the location of the number\n",
    "        image[batch_binary[i]] = 1 - image[batch_binary[i]]\n",
    "\n",
    "        batch[i] = cv2.resize(image, (0, 0), fx=28 / 64, fy=28 / 64, interpolation=cv2.INTER_AREA)\n",
    "    return batch.transpose(0, 3, 1, 2)\n",
    "\n",
    "def load_q4_data():\n",
    "    train, _ = load_q3_data()\n",
    "    mnist = np.array(train.data.reshape(-1, 28, 28, 1) / 255.0)\n",
    "    colored_mnist = get_colored_mnist(mnist)\n",
    "    return mnist.transpose(0, 3, 1, 2), colored_mnist\n",
    "\n",
    "def visualize_cyclegan_datasets():\n",
    "    mnist, colored_mnist = load_q4_data()\n",
    "    mnist, colored_mnist = mnist[:100], colored_mnist[:100]\n",
    "    show_samples(mnist.reshape([100, 28, 28, 1]) * 255.0, title=f'MNIST samples')\n",
    "    show_samples(colored_mnist.transpose([0, 2, 3, 1]) * 255.0, title=f'Colored MNIST samples')\n",
    "\n",
    "visualize_cyclegan_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Za8w7ddOdh7"
   },
   "source": [
    "In [CycleGAN](https://arxiv.org/pdf/1703.10593.pdf), the goal is to learn functions $F$ and $G$ that can transform images from $X \\rightarrow Y$ and vice-versa. This is an unconstrained problem, so we additionally enforce the *cycle-consistency* property, where we want \n",
    "$$x \\approx G(F(x))$$\n",
    "and  \n",
    "$$y \\approx F(G(x))$$\n",
    "This loss function encourages $F$ and $G$ to approximately invert each other. In addition to this cycle-consistency loss, we also have a standard GAN loss such that $F(x)$ and $G(y)$ look like real images from the other domain. \n",
    "\n",
    "Since this is a bonus question, we won't do much hand-holding. We recommend reading through the original paper to get a sense of what architectures and hyperparameters are useful. Note that our datasets are fairly simple, so you won't need excessively large models. \n",
    "\n",
    "**You will report the following deliverables**\n",
    "1. A set of images showing real MNIST digits, transformations of those images into Colored MNIST digits, and reconstructions back into the greyscale domain. \n",
    "2. A set of images showing real Colored MNIST digits, transformations of those images, and reconstructions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8adRfm9vPnen"
   },
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B99F5FjbPqtl"
   },
   "outputs": [],
   "source": [
    "def q4(mnist_data, cmnist_data):\n",
    "    \"\"\"\n",
    "    mnist_data: An (60000, 1, 28, 28) numpy array of black and white images with values in [0, 1]\n",
    "    cmnist_data: An (60000, 3, 28, 28) numpy array of colored images with values in [0, 1]\n",
    "\n",
    "    Returns\n",
    "    - a (20, 28, 28, 1) numpy array of real MNIST digits, in [0, 1]\n",
    "    - a (20, 28, 28, 3) numpy array of translated Colored MNIST digits, in [0, 1]\n",
    "    - a (20, 28, 28, 1) numpy array of reconstructed MNIST digits, in [0, 1]\n",
    "\n",
    "    - a (20, 28, 28, 3) numpy array of real Colored MNIST digits, in [0, 1]\n",
    "    - a (20, 28, 28, 1) numpy array of translated MNIST digits, in [0, 1]\n",
    "    - a (20, 28, 28, 3) numpy array of reconstructed Colored MNIST digits, in [0, 1]\n",
    "    \"\"\"\n",
    "    \"\"\" YOUR CODE HERE \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NIO0hzZ8PpPr"
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "colab_type": "code",
    "id": "-hlvbDvUOxp6",
    "outputId": "d30ae0d8-1292-4557-fd7f-9d8b08009590"
   },
   "outputs": [],
   "source": [
    "def q4_save_results(fn):\n",
    "    mnist, cmnist = load_q4_data()\n",
    "\n",
    "    m1, c1, m2, c2, m3, c3 = fn(mnist, cmnist)\n",
    "    m1, m2, m3 = m1.repeat(3, axis=3), m2.repeat(3, axis=3), m3.repeat(3, axis=3)\n",
    "    mnist_reconstructions = np.concatenate([m1, c1, m2], axis=0)\n",
    "    colored_mnist_reconstructions = np.concatenate([c2, m3, c3], axis=0)\n",
    "\n",
    "    show_samples(mnist_reconstructions * 255.0, nrow=20,\n",
    "                 fname=None,\n",
    "                 title=f'Source domain: MNIST')\n",
    "    show_samples(colored_mnist_reconstructions * 255.0, nrow=20,\n",
    "                 fname=None,\n",
    "                 title=f'Source domain: Colored MNIST')\n",
    "    pass\n",
    "\n",
    "q4_save_results(q4)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "module3_gans.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "037b67dcc11940a5ad2c38b830faf71f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0ee102f0e64e4134b7b4197e0ffed618": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "149ae0a3a02b4f24850539870f170f64": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "14b8fe0143c44ab890480cf61fcee8cc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e4b7dffd7674663aa4317d96e07a425": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4b0dbebfb55e409fa1439e9b41afae66",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_537d532842b34fc28e7fc11f2d458c83",
      "value": 1
     }
    },
    "21a3795bb6c44aa2b2f5de9ccb667bb6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1e4b7dffd7674663aa4317d96e07a425",
       "IPY_MODEL_97c3c0371b43488ba5e628e2aea1fff9"
      ],
      "layout": "IPY_MODEL_77360e5cb1324395831f617a74a14d31"
     }
    },
    "33f612e221bd4ac3a4eba2bdd6493d04": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_53be47757c9941b18366b4d8373cc40a",
       "IPY_MODEL_ed226d6072ae4a65b1b789912593dae5"
      ],
      "layout": "IPY_MODEL_e13c2483c2db47dabfcbc3a7a6de4b35"
     }
    },
    "4b0dbebfb55e409fa1439e9b41afae66": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4fed3c34233c4e2b8e06fe3d8b2b3c8b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "537d532842b34fc28e7fc11f2d458c83": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "53be47757c9941b18366b4d8373cc40a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ae5a8f4cb4d1464e9795f3dbf7dd8691",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_690de1ab3502431d988da8ad8399c838",
      "value": 1
     }
    },
    "553d87b223404f3698ece12862c6ccd7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b4c12d0ef09c41b79000af4573de594b",
       "IPY_MODEL_b34330df934d470f9f1a669c77160386"
      ],
      "layout": "IPY_MODEL_037b67dcc11940a5ad2c38b830faf71f"
     }
    },
    "690de1ab3502431d988da8ad8399c838": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "73f63317156a49cc96eba50e275b4fac": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "77360e5cb1324395831f617a74a14d31": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "95aeb90ff17e42d5bf2c1cd0451f2d68": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "97c3c0371b43488ba5e628e2aea1fff9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4fed3c34233c4e2b8e06fe3d8b2b3c8b",
      "placeholder": "",
      "style": "IPY_MODEL_cefde845a29c442dbdd206067fce84ce",
      "value": " 9920512/? [00:03&lt;00:00, 2666728.22it/s]"
     }
    },
    "9859004091154a18b436fd29a6870aec": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b21abaaeac8144af9ff5661d320fc457",
      "placeholder": "",
      "style": "IPY_MODEL_0ee102f0e64e4134b7b4197e0ffed618",
      "value": " 1654784/? [00:02&lt;00:00, 653718.06it/s]"
     }
    },
    "9f6fd397241645539b88e7cbd2f33dbb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "ae5a8f4cb4d1464e9795f3dbf7dd8691": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b21abaaeac8144af9ff5661d320fc457": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b34330df934d470f9f1a669c77160386": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_73f63317156a49cc96eba50e275b4fac",
      "placeholder": "",
      "style": "IPY_MODEL_149ae0a3a02b4f24850539870f170f64",
      "value": " 8192/? [00:00&lt;00:00, 14156.12it/s]"
     }
    },
    "b4c12d0ef09c41b79000af4573de594b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c597443e923b496ab5a868c0f11cc64b",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9f6fd397241645539b88e7cbd2f33dbb",
      "value": 1
     }
    },
    "c21f4bf579dd47f3b33c84fc76ed3dc9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c597443e923b496ab5a868c0f11cc64b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "caca3df014bb4f1aadac830ac6900e4c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cefde845a29c442dbdd206067fce84ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dc5594291f7a4e27a7820d2402db7254": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e13c2483c2db47dabfcbc3a7a6de4b35": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5a15d3d384e4c0bb0e6d4c6722a4f08": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc5594291f7a4e27a7820d2402db7254",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_95aeb90ff17e42d5bf2c1cd0451f2d68",
      "value": 1
     }
    },
    "ed226d6072ae4a65b1b789912593dae5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_14b8fe0143c44ab890480cf61fcee8cc",
      "placeholder": "",
      "style": "IPY_MODEL_c21f4bf579dd47f3b33c84fc76ed3dc9",
      "value": " 32768/? [00:00&lt;00:00, 49868.92it/s]"
     }
    },
    "f3955b7b0b04470990dab09f9f36dd1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e5a15d3d384e4c0bb0e6d4c6722a4f08",
       "IPY_MODEL_9859004091154a18b436fd29a6870aec"
      ],
      "layout": "IPY_MODEL_caca3df014bb4f1aadac830ac6900e4c"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
